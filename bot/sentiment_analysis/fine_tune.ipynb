{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-03-16T23:57:33.723658Z",
     "iopub.status.busy": "2024-03-16T23:57:33.723267Z",
     "iopub.status.idle": "2024-03-16T23:57:35.229179Z",
     "shell.execute_reply": "2024-03-16T23:57:35.228635Z"
    },
    "id": "n5kH6OUjdubg",
    "outputId": "64186f8c-7b95-401c-85c1-1bd71e2c0067",
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:34.644815Z",
     "start_time": "2024-04-23T05:33:30.610866Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig\n",
    "from torch import tensor\n",
    "from torcheval.metrics.functional import multiclass_f1_score, multiclass_accuracy\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments, DataCollatorWithPadding\n",
    ")\n",
    "from trl import SFTTrainer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-16T23:57:35.230819Z",
     "iopub.status.busy": "2024-03-16T23:57:35.230661Z",
     "iopub.status.idle": "2024-03-16T23:57:35.234878Z",
     "shell.execute_reply": "2024-03-16T23:57:35.234632Z"
    },
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:34.657708Z",
     "start_time": "2024-04-23T05:33:34.648165Z"
    }
   },
   "source": [
    "# prevent env load failed\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-16T23:57:35.236038Z",
     "iopub.status.busy": "2024-03-16T23:57:35.235956Z",
     "iopub.status.idle": "2024-03-16T23:57:35.484723Z",
     "shell.execute_reply": "2024-03-16T23:57:35.483075Z"
    },
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:35.383936Z",
     "start_time": "2024-04-23T05:33:34.659275Z"
    }
   },
   "source": [
    "login(token=os.environ.get(\"HF_TOKEN\", \"\"), add_to_git_credential=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/hermeschen/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-16T23:57:35.502423Z",
     "iopub.status.busy": "2024-03-16T23:57:35.502277Z",
     "iopub.status.idle": "2024-03-16T23:57:39.768709Z",
     "shell.execute_reply": "2024-03-16T23:57:39.768215Z"
    },
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:49.423225Z",
     "start_time": "2024-04-23T05:33:35.387278Z"
    }
   },
   "source": "dataset = load_dataset(\"benjaminbeilharz/better_daily_dialog\", num_proc=16, trust_remote_code=True).remove_columns([\"dialog_id\", \"turn_type\"])",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-16T23:57:39.780835Z",
     "iopub.status.busy": "2024-03-16T23:57:39.780734Z",
     "iopub.status.idle": "2024-03-16T23:57:39.782710Z",
     "shell.execute_reply": "2024-03-16T23:57:39.782542Z"
    },
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:49.497414Z",
     "start_time": "2024-04-23T05:33:49.494365Z"
    }
   },
   "source": "emotion_labels: list = [\"neutral\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"]",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-16T23:57:39.783665Z",
     "iopub.status.busy": "2024-03-16T23:57:39.783573Z",
     "iopub.status.idle": "2024-03-16T23:57:39.794257Z",
     "shell.execute_reply": "2024-03-16T23:57:39.794022Z"
    },
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:49.594653Z",
     "start_time": "2024-04-23T05:33:49.498896Z"
    }
   },
   "source": [
    "dataset = dataset.map(lambda samples: {\n",
    "    \"label\": [emotion_labels[sample] for sample in samples[\"emotion\"]],\n",
    "    \"text\": [sample.strip() for sample in samples[\"utterance\"]]\n",
    "}, remove_columns=[\"utterance\", \"emotion\"], batched=True, num_proc=16)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-16T23:57:39.795542Z",
     "iopub.status.busy": "2024-03-16T23:57:39.795281Z",
     "iopub.status.idle": "2024-03-16T23:57:39.797241Z",
     "shell.execute_reply": "2024-03-16T23:57:39.797093Z"
    },
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:49.602316Z",
     "start_time": "2024-04-23T05:33:49.596096Z"
    }
   },
   "source": "dataset[\"train\"][0]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'neutral',\n",
       " 'text': 'Say , Jim , how about going for a few beers after dinner ?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:49.606971Z",
     "start_time": "2024-04-23T05:33:49.603803Z"
    }
   },
   "cell_type": "code",
   "source": "base_model_name: str = \"michellejieli/emotion_text_classifier\"",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-16T23:57:39.880713Z",
     "iopub.status.busy": "2024-03-16T23:57:39.880651Z",
     "iopub.status.idle": "2024-03-16T23:57:40.123038Z",
     "shell.execute_reply": "2024-03-16T23:57:40.122721Z"
    },
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:50.135489Z",
     "start_time": "2024-04-23T05:33:49.608551Z"
    }
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.truncation_side = \"right\"\n",
    "tokenizer.padding_side = \"right\""
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:50.141019Z",
     "start_time": "2024-04-23T05:33:50.138269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def formatting_function(sample: str) -> tensor:\n",
    "    return tokenizer.encode(sample, truncation=True)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:50.297627Z",
     "start_time": "2024-04-23T05:33:50.142103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = dataset.map(lambda samples: {\n",
    "    \"tokenized_text\": [formatting_function(sample) for sample in samples]\n",
    "}, input_columns=\"text\", batched=True, num_proc=16)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-16T23:57:40.124510Z",
     "iopub.status.busy": "2024-03-16T23:57:40.124435Z",
     "iopub.status.idle": "2024-03-16T23:57:40.126589Z",
     "shell.execute_reply": "2024-03-16T23:57:40.126420Z"
    },
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:50.302102Z",
     "start_time": "2024-04-23T05:33:50.298463Z"
    }
   },
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-16T23:57:40.127624Z",
     "iopub.status.busy": "2024-03-16T23:57:40.127535Z",
     "iopub.status.idle": "2024-03-16T23:57:40.128904Z",
     "shell.execute_reply": "2024-03-16T23:57:40.128749Z"
    },
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:50.306371Z",
     "start_time": "2024-04-23T05:33:50.303631Z"
    }
   },
   "source": [
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-16T23:57:40.129883Z",
     "iopub.status.busy": "2024-03-16T23:57:40.129779Z",
     "iopub.status.idle": "2024-03-16T23:57:40.130993Z",
     "shell.execute_reply": "2024-03-16T23:57:40.130849Z"
    },
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:50.310187Z",
     "start_time": "2024-04-23T05:33:50.308035Z"
    }
   },
   "source": [
    "num_train_epochs: int = 3"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:50.314526Z",
     "start_time": "2024-04-23T05:33:50.311538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(prediction):\n",
    "    outputs, labels = prediction\n",
    "    predictions = torch.argmax(outputs, dim=-1)\n",
    "    \n",
    "    f1_score = multiclass_f1_score(labels, predictions, num_classes=len(emotion_labels), average=\"micro\"),\n",
    "    accuracy = multiclass_accuracy(labels, predictions, num_classes=len(emotion_labels))\n",
    "    \n",
    "    return {\"accuracy\": accuracy, \"f1\": f1_score}"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:50.335243Z",
     "start_time": "2024-04-23T05:33:50.315449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer_arguments = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_accumulation_steps=1,\n",
    "    eval_delay=100,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.001,\n",
    "    # max_grad_norm=0.3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    # lr_scheduler_type=\"constant\",\n",
    "    warmup_ratio=0.03,\n",
    "    max_steps=-1,\n",
    "    logging_steps=25,\n",
    "    save_steps=25,\n",
    "    save_total_limit=5,\n",
    "    bf16=False,\n",
    "    fp16=False,\n",
    "    eval_steps=25,\n",
    "    dataloader_num_workers=16,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    group_by_length=True,\n",
    "    # report_to=[\"wandb\"],\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
    "    auto_find_batch_size=True,\n",
    "    torch_compile=False,\n",
    "    resume_from_checkpoint=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:52.532651Z",
     "start_time": "2024-04-23T05:33:50.336068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    # quantization_config=quantization_config if torch.cuda.is_available() else None,\n",
    "    num_labels=len(emotion_labels),\n",
    "    # is_decoder=True,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at michellejieli/emotion_text_classifier and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T05:33:52.536466Z",
     "start_time": "2024-04-23T05:33:52.533868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "base_model.tra"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup Tuner"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T05:30:33.567241Z",
     "start_time": "2024-04-23T05:30:33.565106Z"
    }
   },
   "cell_type": "code",
   "source": "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"max_length\", return_tensors=\"pt\")",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508,
     "referenced_widgets": [
      "636a931c33cd415ba1eb6c4721080fd2",
      "dda7944038c34f70803c926436ffcfaf",
      "d25d9974d04344509944bbea7b8e3268",
      "8a836fa75d6b400fb3187ec1bf87946a",
      "f5404ba31d8a4295b9da0c26db9b2439",
      "1f14aff2634a48b493cd94733c356a2a",
      "edcdc867e2f44851a25fca997a0944de",
      "1ed8684669ed48629790b5b902c577ba",
      "c55e6bd1be2643d1b846f2d7b8cbf234",
      "d03a30aa2db84d29a8c4a3e34466644d",
      "12c4eef0a8c640349e257d0973a58a41"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-03-16T23:57:43.987566Z",
     "iopub.status.busy": "2024-03-16T23:57:43.987450Z",
     "iopub.status.idle": "2024-03-16T23:57:44.057094Z",
     "shell.execute_reply": "2024-03-16T23:57:44.056880Z"
    },
    "id": "TSsPNqspeFt5",
    "outputId": "58856f44-014b-478e-edd1-fef917d76766",
    "ExecuteTime": {
     "end_time": "2024-04-23T05:30:35.731937Z",
     "start_time": "2024-04-23T05:30:33.568494Z"
    }
   },
   "source": [
    "tuner = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    dataset_text_field=\"tokenized_text\",\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    # formatting_func=formatting_function,\n",
    "    peft_config=peft_parameters,\n",
    "    args=trainer_arguments,\n",
    "    max_seq_length=512,\n",
    "    dataset_num_proc=16,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hermeschen/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/87170 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aea17eda73d241ad9ab04f484bfd8e0c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRemoteTraceback\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;31mRemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/hermeschen/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/hermeschen/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/datasets/utils/py_utils.py\", line 623, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"/Users/hermeschen/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3482, in _map_single\n    batch = apply_function_on_filtered_inputs(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/hermeschen/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3361, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/hermeschen/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/trl/trainer/sft_trainer.py\", line 417, in tokenize\n    outputs = tokenizer(\n              ^^^^^^^^^^\n  File \"/Users/hermeschen/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2829, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/hermeschen/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2887, in _call_one\n    raise ValueError(\nValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m tuner \u001B[38;5;241m=\u001B[39m \u001B[43mSFTTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalidation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset_text_field\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtokenized_text\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_collator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# formatting_func=formatting_function,\u001B[39;49;00m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpeft_parameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrainer_arguments\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_seq_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset_num_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:262\u001B[0m, in \u001B[0;36mSFTTrainer.__init__\u001B[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs)\u001B[0m\n\u001B[1;32m    260\u001B[0m     dataset_kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m train_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 262\u001B[0m     train_dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_dataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    264\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    265\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpacking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    266\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset_text_field\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    267\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_seq_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m        \u001B[49m\u001B[43mformatting_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_of_sequences\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchars_per_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremove_unused_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mremove_unused_columns\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdataset_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m eval_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    275\u001B[0m     _multiple \u001B[38;5;241m=\u001B[39m \u001B[38;5;28misinstance\u001B[39m(eval_dataset, \u001B[38;5;28mdict\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:379\u001B[0m, in \u001B[0;36mSFTTrainer._prepare_dataset\u001B[0;34m(self, dataset, tokenizer, packing, dataset_text_field, max_seq_length, formatting_func, num_of_sequences, chars_per_token, remove_unused_columns, append_concat_token, add_special_tokens)\u001B[0m\n\u001B[1;32m    376\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dataset\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m packing:\n\u001B[0;32m--> 379\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_non_packed_dataloader\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset_text_field\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_seq_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m        \u001B[49m\u001B[43mformatting_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremove_unused_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    387\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    389\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    390\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_packed_dataloader(\n\u001B[1;32m    391\u001B[0m         tokenizer,\n\u001B[1;32m    392\u001B[0m         dataset,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    399\u001B[0m         add_special_tokens,\n\u001B[1;32m    400\u001B[0m     )\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:447\u001B[0m, in \u001B[0;36mSFTTrainer._prepare_non_packed_dataloader\u001B[0;34m(self, tokenizer, dataset, dataset_text_field, max_seq_length, formatting_func, add_special_tokens, remove_unused_columns)\u001B[0m\n\u001B[1;32m    441\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m remove_unused_columns \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(extra_columns) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    442\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    443\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou passed `remove_unused_columns=False` on a non-packed dataset. This might create some issues with the default collator and yield to errors. If you want to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    444\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minspect dataset other columns (in this case \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mextra_columns\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m), you can subclass `DataCollatorForLanguageModeling` in case you used the default collator and create your own data collator in order to inspect the unused dataset columns.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    445\u001B[0m     )\n\u001B[0;32m--> 447\u001B[0m tokenized_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    448\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    449\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    450\u001B[0m \u001B[43m    \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumn_names\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mremove_unused_columns\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    451\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset_num_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    452\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    453\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tokenized_dataset\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/datasets/arrow_dataset.py:593\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    591\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    592\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 593\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    594\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[1;32m    596\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/datasets/arrow_dataset.py:558\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    551\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    552\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[1;32m    553\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[1;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[1;32m    555\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[1;32m    556\u001B[0m }\n\u001B[1;32m    557\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 558\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    559\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    560\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/datasets/arrow_dataset.py:3197\u001B[0m, in \u001B[0;36mDataset.map\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[1;32m   3191\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpawning \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_proc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m processes\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   3192\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m hf_tqdm(\n\u001B[1;32m   3193\u001B[0m     unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3194\u001B[0m     total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[1;32m   3195\u001B[0m     desc\u001B[38;5;241m=\u001B[39m(desc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (num_proc=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_proc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3196\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[0;32m-> 3197\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miflatmap_unordered\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3198\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpool\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_single\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs_iterable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs_per_job\u001B[49m\n\u001B[1;32m   3199\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   3200\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   3201\u001B[0m \u001B[43m            \u001B[49m\u001B[43mshards_done\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/datasets/utils/py_utils.py:663\u001B[0m, in \u001B[0;36miflatmap_unordered\u001B[0;34m(pool, func, kwargs_iterable)\u001B[0m\n\u001B[1;32m    660\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    661\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m pool_changed:\n\u001B[1;32m    662\u001B[0m         \u001B[38;5;66;03m# we get the result in case there's an error to raise\u001B[39;00m\n\u001B[0;32m--> 663\u001B[0m         \u001B[43m[\u001B[49m\u001B[43masync_result\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.05\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43masync_result\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43masync_results\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/datasets/utils/py_utils.py:663\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    660\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    661\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m pool_changed:\n\u001B[1;32m    662\u001B[0m         \u001B[38;5;66;03m# we get the result in case there's an error to raise\u001B[39;00m\n\u001B[0;32m--> 663\u001B[0m         [\u001B[43masync_result\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.05\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m async_result \u001B[38;5;129;01min\u001B[39;00m async_results]\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/chat-bot-4GJbbAtJ-py3.11/lib/python3.11/site-packages/multiprocess/pool.py:774\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    772\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n\u001B[1;32m    773\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 774\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n",
      "\u001B[0;31mValueError\u001B[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tuner.predict(dataset[\"test\"])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-16T23:57:44.058478Z",
     "iopub.status.busy": "2024-03-16T23:57:44.058352Z",
     "iopub.status.idle": "2024-03-17T17:29:31.299778Z",
     "shell.execute_reply": "2024-03-17T17:29:31.299586Z"
    }
   },
   "source": [
    "tuner.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-17T17:29:31.307746Z",
     "iopub.status.busy": "2024-03-17T17:29:31.307452Z",
     "iopub.status.idle": "2024-03-17T17:29:31.742306Z",
     "shell.execute_reply": "2024-03-17T17:29:31.741999Z"
    }
   },
   "source": [
    "tuner.model = torch.compile(tuner.model)\n",
    "tuner.save_model(\"./model\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2b7f52a715924339b2d2ca3d5a0fc02d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2bd2c1dc7b7447eca8c01a9ed88a6001": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9545dbca917d4c529de8cab5e88ab802",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c0bf096919ce437e82366fab1aed6d66",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "6190f0ee16244d5f80c2145654259e51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9545dbca917d4c529de8cab5e88ab802": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9955ff5df3b94d6f9d2b6e27df304686": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e6c4ff3fa1d5475fb22b605b702c5155",
       "placeholder": "​",
       "style": "IPY_MODEL_b74da2f5e9694f8ab607998a522d91ce",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "b48789d2c20a4140b447ae28f84913d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2b7f52a715924339b2d2ca3d5a0fc02d",
       "placeholder": "​",
       "style": "IPY_MODEL_6190f0ee16244d5f80c2145654259e51",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [00:03&lt;00:00,  1.42s/it]"
      }
     },
     "b74da2f5e9694f8ab607998a522d91ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c0bf096919ce437e82366fab1aed6d66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c1399fbbd87a43caa83b8d25b83f4ccc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6c4ff3fa1d5475fb22b605b702c5155": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f4010bea48cf46569f8f2578a7830461": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9955ff5df3b94d6f9d2b6e27df304686",
        "IPY_MODEL_2bd2c1dc7b7447eca8c01a9ed88a6001",
        "IPY_MODEL_b48789d2c20a4140b447ae28f84913d0"
       ],
       "layout": "IPY_MODEL_c1399fbbd87a43caa83b8d25b83f4ccc",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
