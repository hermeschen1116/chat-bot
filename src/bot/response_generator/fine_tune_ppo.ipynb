{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from dataclasses import dataclass, Field\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from bitsandbytes.optim import PagedLion32bit\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from peft.peft_model import PeftModel\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers import (\n",
    "\tBitsAndBytesConfig,\n",
    "\tGenerationConfig,\n",
    "\tHfArgumentParser,\n",
    "\tpipeline, TextStreamer\n",
    ")\n",
    "from transformers.hf_argparser import HfArg\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "from libs import CommonScriptArguments, CommonWanDBArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @dataclass\n",
    "# class ScriptArguments(CommonScriptArguments):\n",
    "# \tchat_template_file: Field[str] = HfArg(aliases=\"--chat-template-file\", default=\"\")\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments(CommonScriptArguments):\n",
    "    chat_template_file: str = HfArg(aliases=\"--chat-template-file\", default=\"\")\n",
    "\n",
    "\n",
    "\n",
    "# config_getter = ArgumentParser()\n",
    "# config_getter.add_argument(\"--json_file\", required=True, type=str)\n",
    "# config = config_getter.parse_args()\n",
    "\n",
    "parser = HfArgumentParser((ScriptArguments, CommonWanDBArguments))\n",
    "args, wandb_args = parser.parse_json_file(\"/home/user/github/chat-bot/src/bot/response_generator/args/ppo_arg.json\")\n",
    "# args, wandb_args = parser.parse_json_file(config.json_file)\n",
    "\n",
    "chat_template: dict = eval(open(args.chat_template_file, \"r\", encoding=\"utf-8\", closefd=True).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wandb\n",
    "run = wandb.init(\n",
    "\tjob_type=wandb_args.job_type,\n",
    "\tconfig=wandb_args.config,\n",
    "\tproject=wandb_args.project,\n",
    "\tgroup=wandb_args.group,\n",
    "\tnotes=wandb_args.notes,\n",
    "\tmode=wandb_args.mode,\n",
    "\tresume=wandb_args.resume\n",
    ")\n",
    "wandb.config[\"chat_template\"] = chat_template[\"template\"]\n",
    "wandb.config[\"instruction_template\"] = chat_template[\"instruction\"]\n",
    "wandb.config[\"response_template\"] = chat_template[\"response\"]\n",
    "wandb.config[\"special_tokens\"] = chat_template[\"special_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "dataset = load_dataset(\"hermeschen1116/daily_dialog_for_RG\", num_proc=16, trust_remote_code=True)\n",
    "dataset = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]])\n",
    "dataset = dataset.train_test_split(train_size=0.05)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_length: int = 2 * wandb.config[\"num_turns_history\"]\n",
    "dataset = dataset.filter(lambda sample: len(sample) >= (2 + history_length), input_columns=\"prompt\", num_proc=16)\n",
    "print(f\"dataset size after filter: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda sample: {\n",
    "\t\"prompt\": sample[i: i + 2] for i in range(0, len(sample) - 2, 2)\n",
    "}, input_columns=\"prompt\", batched=False, num_proc=16)\n",
    "\n",
    "system_prompt: list = [{\"role\": \"system\", \"content\": {\"emotion\": \"\", \"dialog\": wandb.config[\"system_prompt\"]}}]\n",
    "\n",
    "dataset = dataset.map(lambda samples: {\n",
    "\t\"prompt\": [system_prompt + sample for sample in samples]\n",
    "}, input_columns=\"prompt\", batched=True, num_proc=16)\n",
    "\n",
    "emotion_labels: list = [\"neutral\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"]\n",
    "\n",
    "dataset = dataset.map(lambda samples: {\n",
    "\t\"query\": [\n",
    "\t\tsample[:-1] + [{\"role\": \"assistant\", \"content\": {\"emotion\": sample[-1][\"content\"][\"emotion\"], \"dialog\": \"\"}}]\n",
    "\t\tfor sample in samples\n",
    "\t],\n",
    "\t\"label\": [emotion_labels.index(sample[-1][\"content\"][\"emotion\"]) for sample in samples]\n",
    "}, input_columns=\"prompt\", remove_columns=\"prompt\", batched=True, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "\twandb.config[\"base_model\"],\n",
    "\tattn_implementation=\"flash_attention_2\",\n",
    "\tpretraining_tp=1,\n",
    "\tload_in_4bit=True,\n",
    "\tuse_cache=False,\n",
    "\tdevice_map=\"auto\",\n",
    "\tuse_gradient_checkpointing=True,\n",
    "\tlow_cpu_mem_usage=True,\n",
    "\ttrust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.clean_up_tokenization_spaces = True\n",
    "tokenizer.chat_template = wandb.config[\"chat_template\"]\n",
    "tokenizer.add_special_tokens(wandb.config[\"special_tokens\"])\n",
    "base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_with_adapter = PeftModel.from_pretrained(base_model, wandb.config[\"adapter\"])\n",
    "base_model_with_adapter.print_trainable_parameters()\n",
    "FastLanguageModel.for_inference(base_model_with_adapter)\n",
    "\n",
    "base_model_with_adapter = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "\tbase_model_with_adapter,\n",
    "\tdevice_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda sample: {\n",
    "\t\"input_ids\": tokenizer.apply_chat_template(\n",
    "\t\tsample,\n",
    "\t\ttokenize=True,\n",
    "\t\tpadding=\"max_length\",\n",
    "\t\tmax_length=wandb.config[\"max_input_tokens\"],\n",
    "\t\tadd_generation_prompt=True,\n",
    "\t\treturn_tensors=\"pt\"\n",
    "\t)[0]\n",
    "}, input_columns=\"query\", num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "analyser = pipeline(\n",
    "\tmodel=wandb.config[\"sentiment_analysis_model\"],\n",
    "\tframework=\"pt\",\n",
    "\ttask=\"sentiment-analysis\",\n",
    "\tnum_workers=16,\n",
    "\tdevice_map=\"auto\",\n",
    "\ttorch_dtype=\"auto\",\n",
    "\tmodel_kwargs={\n",
    "\t\t\"quantization_config\": BitsAndBytesConfig(\n",
    "\t\t\tload_in_4bit=True,\n",
    "\t\t\tbnb_4bit_compute_dtype=torch.float16\n",
    "\t\t),\n",
    "\t\t\"id2label\": {k: v for k, v in enumerate(emotion_labels)},\n",
    "\t\t\"label2id\": {v: k for k, v in enumerate(emotion_labels)},\n",
    "\t\t\"low_cpu_mem_usage\": True\n",
    "\t},\n",
    "\ttrust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_model = torch.compile(analyser.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_length = 69\n",
    "def calculate_emotion_score(response: str, correct_emotion: str) -> float:\n",
    "    # correct: save the score from analyser \n",
    "    # wrong: [TO-DO] (save 1 - score from analyser )\n",
    "    emotion_output = analyser(response)[0]\n",
    "    if emotion_output[\"label\"] == correct_emotion:\n",
    "        emotion_score = emotion_output[\"score\"] * 10\n",
    "    else:\n",
    "        emotion_score = 1 - emotion_output[\"score\"]\n",
    "    return emotion_score\n",
    "\n",
    "def calculate_length_score(response: str) -> float:\n",
    "    # use reciprocal of length difference to calculate\n",
    "    # the larger the difference the smaller the score is\n",
    "    length_diff = abs(len(response) - target_length)\n",
    "    print(length_diff)\n",
    "    length_score = 1 / (length_diff + 1)\n",
    "    return length_score\n",
    "\n",
    "def reward(batch: dict) -> list:\n",
    "    print(\"Hello Huston, here is a reward function\")\n",
    "    correct_emotion = batch['query'][2]['content']['emotion']\n",
    "    print(correct_emotion)\n",
    "    rewards = []\n",
    "    for response in batch[\"response\"]:\n",
    "        emotion_score = calculate_emotion_score(response, correct_emotion)\n",
    "        length_score = calculate_length_score(response)\n",
    "        # use the product of two score as reward\n",
    "        reward_product = emotion_score * length_score\n",
    "        rewards.append(reward_product)\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_batch = {\n",
    "    \"query\": [\n",
    "        {\"content\": {\"text\": \"Hello, how are you?\", \"emotion\": \"sadness\"}},\n",
    "        {\"content\": {\"text\": \"It's a nice day today.\", \"emotion\": \"neutral\"}},\n",
    "        {\"content\": {\"text\": \"\", \"emotion\": \"anger\"}}\n",
    "    ],\n",
    "    \"response\": [\n",
    "        \"I am doing well, thank you!\",\n",
    "        \"Yes, it is a beautiful day.\",\n",
    "        \"Man, wtf was that???? OK, I'm leaving now! Go fuck yourself and your bullshit.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "rewards = reward(mock_batch)\n",
    "print(\"Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = PPOConfig(\n",
    "\tgradient_accumulation_steps=1,\n",
    "\tlearning_rate=wandb.config[\"learning_rate\"],\n",
    "\tmax_grad_norm=wandb.config[\"max_grad_norm\"],\n",
    "\tlog_with=\"wandb\",\n",
    "\toptimize_device_cache=True,\n",
    "\tearly_stopping=True,\n",
    "\tis_peft_model=True,\n",
    "\tuse_score_scaling=True,\n",
    "\tuse_score_norm=True,\n",
    "\tscore_clip=wandb.config[\"score_clip\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = PagedLion32bit(filter(lambda p: p.requires_grad, base_model.parameters()), lr=ppo_config.learning_rate)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(\n",
    "\ttokenizer,\n",
    "\tskip_special_tokens=True, # show <pad> or not\n",
    "\tclean_up_tokenization_spaces=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "\tmax_length=(wandb.config[\"max_input_tokens\"] + wandb.config[\"max_new_tokens\"]),\n",
    "\tmin_length=-1,\n",
    "\ttop_k=wandb.config[\"top_k\"],\n",
    "\ttop_p=wandb.config[\"top_p\"],\n",
    "\tdo_sample=True,\n",
    "\trepetition_penalty=wandb.config[\"repetition_penalty\"],\n",
    "\tpad_token_id=tokenizer.pad_token_id,\n",
    "\teos_token_id=tokenizer.eos_token_id,\n",
    "\tlow_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tuner\n",
    "tuner = PPOTrainer(\n",
    "\tconfig=ppo_config,\n",
    "\tmodel=base_model_with_adapter,\n",
    "\ttokenizer=tokenizer,\n",
    "\tdataset=dataset,\n",
    "\toptimizer=optimizer,\n",
    "\tlr_scheduler=lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(1, colour=\"blue\"):\n",
    "\tfor batch in tqdm(tuner.dataloader, colour=\"yellow\"):\n",
    "\t\tquery_tensors = batch[\"input_ids\"]\n",
    "\t\t# print(batch)\n",
    "\t\t# Get response from SFTModel\n",
    "\t\tresponse_tensors = tuner.generate(\n",
    "\t\t\tquery_tensors,\n",
    "\t\t\treturn_prompt=False,\n",
    "\t\t\t# batch_size=1,   # must set to 1 if using streamer\n",
    "\t\t\t# streamer=streamer,  # use streamer to show the generation process\n",
    "\t\t\t**generation_config.to_dict()\n",
    "\t\t)\n",
    "\t\tbatch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\t\tresponse_tensors = [torch.LongTensor(t, device=\"cpu\") for t in response_tensors]\n",
    "\n",
    "\t\t# Compute reward score\n",
    "\t\treward_scores = reward(batch)\n",
    "\t\trewards = [torch.FloatTensor(scores, device=\"cpu\") for scores in reward_scores]\n",
    "\n",
    "\t\t# Run PPO step\n",
    "\t\tstats = tuner.step(query_tensors, response_tensors, rewards)\n",
    "\t\ttuner.log_stats(stats, batch, rewards)\n",
    "\n",
    "# model_artifact = wandb.Artifact(\n",
    "# \twandb.config[\"fine_tuned_model\"],\n",
    "# \ttype=\"model\"\n",
    "# )\n",
    "\n",
    "tuner.model = torch.compile(tuner.model)\n",
    "tuner.model.push_to_hub(repo_id=\"response_generator_for_emotion_chat_bot\", commit=\"\", create_pr=True)\n",
    "# with tempfile.TemporaryDirectory() as temp_dir:\n",
    "# \ttuner.model.save_pretrained(temp_dir, save_embedding_layers=True)\n",
    "# \tmodel_artifact.add_dir(temp_dir)\n",
    "# \trun.log_artifact(model_artifact)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-bot-20tW9agt-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
