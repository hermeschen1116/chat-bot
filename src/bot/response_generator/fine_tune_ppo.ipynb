{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.cache/pypoetry/virtualenvs/chat-bot-20tW9agt-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "from dataclasses import dataclass, Field\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from bitsandbytes.optim import PagedLion32bit\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from peft.peft_model import PeftModel\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers import (\n",
    "\tBitsAndBytesConfig,\n",
    "\tGenerationConfig,\n",
    "\tHfArgumentParser,\n",
    "\tpipeline, TextStreamer\n",
    ")\n",
    "from transformers.hf_argparser import HfArg\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "from libs import CommonScriptArguments, CommonWanDBArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (cache).\n",
      "Your token has been saved to /home/user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/user/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This parameter should be any of Sentiment Analysis, Candidate Generator, Emotion Predictor, Emotion Model, Similarity Analysis, Response Generator, your input is Candidates Generator\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# @dataclass\n",
    "# class ScriptArguments(CommonScriptArguments):\n",
    "# \tchat_template_file: Field[str] = HfArg(aliases=\"--chat-template-file\", default=\"\")\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments(CommonScriptArguments):\n",
    "    chat_template_file: str = HfArg(aliases=\"--chat-template-file\", default=\"\")\n",
    "\n",
    "\n",
    "\n",
    "# config_getter = ArgumentParser()\n",
    "# config_getter.add_argument(\"--json_file\", required=True, type=str)\n",
    "# config = config_getter.parse_args()\n",
    "\n",
    "parser = HfArgumentParser((ScriptArguments, CommonWanDBArguments))\n",
    "args, wandb_args = parser.parse_json_file(\"/home/user/github/chat-bot/src/bot/response_generator/args/ppo_arg.json\")\n",
    "# args, wandb_args = parser.parse_json_file(config.json_file)\n",
    "\n",
    "chat_template: dict = eval(open(args.chat_template_file, \"r\", encoding=\"utf-8\", closefd=True).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myangyx30678\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/github/chat-bot/src/bot/response_generator/wandb/run-20240717_095625-ycoctm0n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yangyx30678/emotion-chat-bot-ncu/runs/ycoctm0n' target=\"_blank\">snowy-tree-118</a></strong> to <a href='https://wandb.ai/yangyx30678/emotion-chat-bot-ncu' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yangyx30678/emotion-chat-bot-ncu' target=\"_blank\">https://wandb.ai/yangyx30678/emotion-chat-bot-ncu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yangyx30678/emotion-chat-bot-ncu/runs/ycoctm0n' target=\"_blank\">https://wandb.ai/yangyx30678/emotion-chat-bot-ncu/runs/ycoctm0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Wandb\n",
    "run = wandb.init(\n",
    "\tjob_type=wandb_args.job_type,\n",
    "\tconfig=wandb_args.config,\n",
    "\tproject=wandb_args.project,\n",
    "\tgroup=wandb_args.group,\n",
    "\tnotes=wandb_args.notes,\n",
    "\tmode=wandb_args.mode,\n",
    "\tresume=wandb_args.resume\n",
    ")\n",
    "wandb.config[\"chat_template\"] = chat_template[\"template\"]\n",
    "wandb.config[\"instruction_template\"] = chat_template[\"instruction\"]\n",
    "wandb.config[\"response_template\"] = chat_template[\"response\"]\n",
    "wandb.config[\"special_tokens\"] = chat_template[\"special_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "dataset = load_dataset(\"hermeschen1116/daily_dialog_for_RG\", num_proc=16, trust_remote_code=True)\n",
    "dataset = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]])\n",
    "dataset = dataset.train_test_split(train_size=0.05)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 605/605 [00:00<00:00, 5532.61 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size after filter: 402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history_length: int = 2 * wandb.config[\"num_turns_history\"]\n",
    "dataset = dataset.filter(lambda sample: len(sample) >= (2 + history_length), input_columns=\"prompt\", num_proc=16)\n",
    "print(f\"dataset size after filter: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 402/402 [00:00<00:00, 3826.63 examples/s]\n",
      "Map (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 402/402 [00:00<00:00, 4120.51 examples/s]\n",
      "Map (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 402/402 [00:00<00:00, 3946.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda sample: {\n",
    "\t\"prompt\": sample[i: i + 2] for i in range(0, len(sample) - 2, 2)\n",
    "}, input_columns=\"prompt\", batched=False, num_proc=16)\n",
    "\n",
    "system_prompt: list = [{\"role\": \"system\", \"content\": {\"emotion\": \"\", \"dialog\": wandb.config[\"system_prompt\"]}}]\n",
    "\n",
    "dataset = dataset.map(lambda samples: {\n",
    "\t\"prompt\": [system_prompt + sample for sample in samples]\n",
    "}, input_columns=\"prompt\", batched=True, num_proc=16)\n",
    "\n",
    "emotion_labels: list = [\"neutral\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"]\n",
    "\n",
    "dataset = dataset.map(lambda samples: {\n",
    "\t\"query\": [\n",
    "\t\tsample[:-1] + [{\"role\": \"assistant\", \"content\": {\"emotion\": sample[-1][\"content\"][\"emotion\"], \"dialog\": \"\"}}]\n",
    "\t\tfor sample in samples\n",
    "\t],\n",
    "\t\"label\": [emotion_labels.index(sample[-1][\"content\"][\"emotion\"]) for sample in samples]\n",
    "}, input_columns=\"prompt\", remove_columns=\"prompt\", batched=True, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.7\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.669 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32005, 4096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Tokenizer\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "\twandb.config[\"base_model\"],\n",
    "\tattn_implementation=\"flash_attention_2\",\n",
    "\tpretraining_tp=1,\n",
    "\tload_in_4bit=True,\n",
    "\tuse_cache=False,\n",
    "\tdevice_map=\"auto\",\n",
    "\tuse_gradient_checkpointing=True,\n",
    "\tlow_cpu_mem_usage=True,\n",
    "\ttrust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.clean_up_tokenization_spaces = True\n",
    "tokenizer.chat_template = wandb.config[\"chat_template\"]\n",
    "tokenizer.add_special_tokens(wandb.config[\"special_tokens\"])\n",
    "base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262,184,960 || all params: 7,020,630,016 || trainable%: 3.7345\n"
     ]
    }
   ],
   "source": [
    "base_model_with_adapter = PeftModel.from_pretrained(base_model, wandb.config[\"adapter\"])\n",
    "base_model_with_adapter.print_trainable_parameters()\n",
    "FastLanguageModel.for_inference(base_model_with_adapter)\n",
    "\n",
    "base_model_with_adapter = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "\tbase_model_with_adapter,\n",
    "\tdevice_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 402/402 [00:00<00:00, 2587.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda sample: {\n",
    "\t\"input_ids\": tokenizer.apply_chat_template(\n",
    "\t\tsample,\n",
    "\t\ttokenize=True,\n",
    "\t\tpadding=\"max_length\",\n",
    "\t\tmax_length=wandb.config[\"max_input_tokens\"],\n",
    "\t\tadd_generation_prompt=True,\n",
    "\t\treturn_tensors=\"pt\"\n",
    "\t)[0]\n",
    "}, input_columns=\"query\", num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "analyser = pipeline(\n",
    "\tmodel=wandb.config[\"sentiment_analysis_model\"],\n",
    "\ttokenizer=wandb.config[\"sentiment_analysis_model\"],\n",
    "\tmax_length=512,\n",
    "\ttruncation=True,\n",
    "\tframework=\"pt\",\n",
    "\ttask=\"sentiment-analysis\",\n",
    "\tnum_workers=16,\n",
    "\tdevice_map=\"auto\",\n",
    "\ttorch_dtype=\"auto\",\n",
    "\tmodel_kwargs={\n",
    "\t\t\"quantization_config\": BitsAndBytesConfig(\n",
    "\t\t\tload_in_4bit=True,\n",
    "\t\t\tbnb_4bit_compute_dtype=torch.float16\n",
    "\t\t),\n",
    "\t\t\"id2label\": {k: v for k, v in enumerate(emotion_labels)},\n",
    "\t\t\"label2id\": {v: k for k, v in enumerate(emotion_labels)},\n",
    "\t\t\"low_cpu_mem_usage\": True\n",
    "\t},\n",
    "\ttrust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_model = torch.compile(analyser.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_length = 69\n",
    "def calculate_emotion_score(response: str, correct_emotion: str) -> float:\n",
    "    # correct: save the score from analyser \n",
    "    # wrong: [TO-DO] (save 1 - score from analyser )\n",
    "    emotion_output = analyser(response)[0]\n",
    "    if emotion_output[\"label\"] == correct_emotion:\n",
    "        emotion_score = emotion_output[\"score\"] * 10\n",
    "    else:\n",
    "        emotion_score = 1 - emotion_output[\"score\"]\n",
    "    return emotion_score\n",
    "\n",
    "def calculate_length_score(response: str) -> float:\n",
    "    # use reciprocal of length difference to calculate\n",
    "    # the larger the difference the smaller the score is\n",
    "    length_diff = abs(len(response) - target_length)\n",
    "    print(length_diff)\n",
    "    length_score = 1 / (length_diff + 1)\n",
    "    return length_score\n",
    "\n",
    "def reward(batch: dict) -> list:\n",
    "    print(\"Hello Huston, here is a reward function\")\n",
    "    correct_emotion = batch['query'][2]['content']['emotion']\n",
    "    print(correct_emotion)\n",
    "    rewards = []\n",
    "    for response in batch[\"response\"]:\n",
    "        emotion_score = calculate_emotion_score(response, correct_emotion)\n",
    "        length_score = calculate_length_score(response)\n",
    "        # use the product of two score as reward\n",
    "        reward_product = emotion_score * length_score\n",
    "        rewards.append(reward_product)\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaTokenizerFast' object has no attribute 'truncation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncation\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaTokenizerFast' object has no attribute 'truncation'"
     ]
    }
   ],
   "source": [
    "tokenizer.truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Huston, here is a reward function\n",
      "anger\n",
      "42\n",
      "42\n",
      "9\n",
      "Rewards: [0.002608583417049674, 0.0009997778160627498, 0.9268014430999756]\n"
     ]
    }
   ],
   "source": [
    "mock_batch = {\n",
    "    \"query\": [\n",
    "        {\"content\": {\"text\": \"Hello, how are you?\", \"emotion\": \"sadness\"}},\n",
    "        {\"content\": {\"text\": \"It's a nice day today.\", \"emotion\": \"neutral\"}},\n",
    "        {\"content\": {\"text\": \"\", \"emotion\": \"anger\"}}\n",
    "    ],\n",
    "    \"response\": [\n",
    "        \"I am doing well, thank you!\",\n",
    "        \"Yes, it is a beautiful day.\",\n",
    "        \"Man, wtf was that???? OK, I'm leaving now! Go fuck yourself and your bullshit.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "rewards = reward(mock_batch)\n",
    "print(\"Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = PPOConfig(\n",
    "\tgradient_accumulation_steps=1,\n",
    "\tlearning_rate=wandb.config[\"learning_rate\"],\n",
    "\tmax_grad_norm=wandb.config[\"max_grad_norm\"],\n",
    "\tlog_with=\"wandb\",\n",
    "\toptimize_device_cache=True,\n",
    "\tearly_stopping=True,\n",
    "\tis_peft_model=True,\n",
    "\tuse_score_scaling=True,\n",
    "\tuse_score_norm=True,\n",
    "\tscore_clip=wandb.config[\"score_clip\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = PagedLion32bit(filter(lambda p: p.requires_grad, base_model.parameters()), lr=ppo_config.learning_rate)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(\n",
    "\ttokenizer,\n",
    "\tskip_special_tokens=True, # show <pad> or not\n",
    "\tclean_up_tokenization_spaces=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "\tmax_length=(wandb.config[\"max_input_tokens\"] + wandb.config[\"max_new_tokens\"]),\n",
    "\tmin_length=-1,\n",
    "\ttop_k=wandb.config[\"top_k\"],\n",
    "\ttop_p=wandb.config[\"top_p\"],\n",
    "\tdo_sample=True,\n",
    "\trepetition_penalty=wandb.config[\"repetition_penalty\"],\n",
    "\tpad_token_id=tokenizer.pad_token_id,\n",
    "\teos_token_id=tokenizer.eos_token_id,\n",
    "\tlow_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tuner\n",
    "tuner = PPOTrainer(\n",
    "\tconfig=ppo_config,\n",
    "\tmodel=base_model_with_adapter,\n",
    "\ttokenizer=tokenizer,\n",
    "\tdataset=dataset,\n",
    "\toptimizer=optimizer,\n",
    "\tlr_scheduler=lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(1, colour=\"blue\"):\n",
    "\tfor batch in tqdm(tuner.dataloader, colour=\"yellow\"):\n",
    "\t\tquery_tensors = batch[\"input_ids\"]\n",
    "\t\t# print(batch)\n",
    "\t\t# Get response from SFTModel\n",
    "\t\tresponse_tensors = tuner.generate(\n",
    "\t\t\tquery_tensors,\n",
    "\t\t\treturn_prompt=False,\n",
    "\t\t\t# batch_size=1,   # must set to 1 if using streamer\n",
    "\t\t\t# streamer=streamer,  # use streamer to show the generation process\n",
    "\t\t\t**generation_config.to_dict()\n",
    "\t\t)\n",
    "\t\tbatch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\t\tresponse_tensors = [torch.LongTensor(t, device=\"cpu\") for t in response_tensors]\n",
    "\n",
    "\t\t# Compute reward score\n",
    "\t\treward_scores = reward(batch)\n",
    "\t\trewards = [torch.FloatTensor(scores, device=\"cpu\") for scores in reward_scores]\n",
    "\n",
    "\t\t# Run PPO step\n",
    "\t\tstats = tuner.step(query_tensors, response_tensors, rewards)\n",
    "\t\ttuner.log_stats(stats, batch, rewards)\n",
    "\n",
    "# model_artifact = wandb.Artifact(\n",
    "# \twandb.config[\"fine_tuned_model\"],\n",
    "# \ttype=\"model\"\n",
    "# )\n",
    "\n",
    "tuner.model = torch.compile(tuner.model)\n",
    "tuner.model.push_to_hub(repo_id=\"response_generator_for_emotion_chat_bot\", commit=\"\", create_pr=True)\n",
    "# with tempfile.TemporaryDirectory() as temp_dir:\n",
    "# \ttuner.model.save_pretrained(temp_dir, save_embedding_layers=True)\n",
    "# \tmodel_artifact.add_dir(temp_dir)\n",
    "# \trun.log_artifact(model_artifact)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-bot-20tW9agt-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
