{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from dataclasses import dataclass, Field\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from bitsandbytes.optim import PagedLion32bit\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from peft.peft_model import PeftModel\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers import (\n",
    "\tBitsAndBytesConfig,\n",
    "\tGenerationConfig,\n",
    "\tHfArgumentParser,\n",
    "\tpipeline, TextStreamer\n",
    ")\n",
    "from transformers.hf_argparser import HfArg\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "from libs import CommonScriptArguments, CommonWanDBArguments, ResponseGeneratorPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (cache).\n",
      "Your token has been saved to /home/user/.cache/huggingface/token\n",
      "Login successful\n",
      "This parameter should be any of Sentiment Analysis, Candidate Generator, Emotion Predictor, Emotion Model, Similarity Analysis, Response Generator, your input is Candidates Generator\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# @dataclass\n",
    "# class ScriptArguments(CommonScriptArguments):\n",
    "# \tchat_template_file: Field[str] = HfArg(aliases=\"--chat-template-file\", default=\"\")\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments(CommonScriptArguments):\n",
    "    chat_template_file: str = HfArg(aliases=\"--chat-template-file\", default=\"\")\n",
    "\n",
    "\n",
    "\n",
    "# config_getter = ArgumentParser()\n",
    "# config_getter.add_argument(\"--json_file\", required=True, type=str)\n",
    "# config = config_getter.parse_args()\n",
    "\n",
    "parser = HfArgumentParser((ScriptArguments, CommonWanDBArguments))\n",
    "args, wandb_args = parser.parse_json_file(\"/home/user/github/chat-bot/src/bot/response_generator/args/ppo_arg.json\")\n",
    "# args, wandb_args = parser.parse_json_file(config.json_file)\n",
    "\n",
    "chat_template: dict = eval(open(args.chat_template_file, \"r\", encoding=\"utf-8\", closefd=True).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ylqj2rf1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fancy-blaze-136</strong> at: <a href='https://wandb.ai/yangyx30678/emotion-chat-bot-ncu/runs/ylqj2rf1' target=\"_blank\">https://wandb.ai/yangyx30678/emotion-chat-bot-ncu/runs/ylqj2rf1</a><br/> View project at: <a href='https://wandb.ai/yangyx30678/emotion-chat-bot-ncu' target=\"_blank\">https://wandb.ai/yangyx30678/emotion-chat-bot-ncu</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240718_121054-ylqj2rf1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ylqj2rf1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/github/chat-bot/src/bot/response_generator/wandb/run-20240718_123508-j071vt0k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yangyx30678/emotion-chat-bot-ncu/runs/j071vt0k' target=\"_blank\">dry-energy-137</a></strong> to <a href='https://wandb.ai/yangyx30678/emotion-chat-bot-ncu' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yangyx30678/emotion-chat-bot-ncu' target=\"_blank\">https://wandb.ai/yangyx30678/emotion-chat-bot-ncu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yangyx30678/emotion-chat-bot-ncu/runs/j071vt0k' target=\"_blank\">https://wandb.ai/yangyx30678/emotion-chat-bot-ncu/runs/j071vt0k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Wandb\n",
    "run = wandb.init(\n",
    "\tjob_type=wandb_args.job_type,\n",
    "\tconfig=wandb_args.config,\n",
    "\tproject=wandb_args.project,\n",
    "\tgroup=wandb_args.group,\n",
    "\tnotes=wandb_args.notes,\n",
    "\tmode=wandb_args.mode,\n",
    "\tresume=wandb_args.resume\n",
    ")\n",
    "wandb.config[\"chat_template\"] = chat_template[\"template\"]\n",
    "wandb.config[\"instruction_template\"] = chat_template[\"instruction\"]\n",
    "wandb.config[\"response_template\"] = chat_template[\"response\"]\n",
    "wandb.config[\"special_tokens\"] = chat_template[\"special_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Dataset\n",
    "# dataset = load_dataset(\"hermeschen1116/daily_dialog_for_RG\", num_proc=16, trust_remote_code=True)\n",
    "# dataset = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]])\n",
    "# dataset = dataset.train_test_split(train_size=0.05)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_length: int = 2 * wandb.config[\"num_turns_history\"]\n",
    "# dataset = dataset.filter(lambda sample: len(sample) >= (2 + history_length), input_columns=\"prompt\", num_proc=16)\n",
    "# print(f\"dataset size after filter: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.map(lambda sample: {\n",
    "# \t\"prompt\": sample[i: i + 2] for i in range(0, len(sample) - 2, 2)\n",
    "# }, input_columns=\"prompt\", batched=False, num_proc=16)\n",
    "\n",
    "# system_prompt: list = [{\"role\": \"system\", \"content\": {\"emotion\": \"\", \"dialog\": wandb.config[\"system_prompt\"]}}]\n",
    "\n",
    "# dataset = dataset.map(lambda samples: {\n",
    "# \t\"prompt\": [system_prompt + sample for sample in samples]\n",
    "# }, input_columns=\"prompt\", batched=True, num_proc=16)\n",
    "\n",
    "# emotion_labels: list = [\"neutral\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"]\n",
    "\n",
    "# dataset = dataset.map(lambda samples: {\n",
    "# \t\"query\": [\n",
    "# \t\tsample[:-1] + [{\"role\": \"assistant\", \"content\": {\"emotion\": sample[-1][\"content\"][\"emotion\"], \"dialog\": \"\"}}]\n",
    "# \t\tfor sample in samples\n",
    "# \t],\n",
    "# \t\"label\": [emotion_labels.index(sample[-1][\"content\"][\"emotion\"]) for sample in samples]\n",
    "# }, input_columns=\"prompt\", remove_columns=\"prompt\", batched=True, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.7\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.669 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32005, 4096)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Tokenizer\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "\twandb.config[\"base_model\"],\n",
    "\tattn_implementation=\"flash_attention_2\",\n",
    "\tpretraining_tp=1,\n",
    "\tload_in_4bit=True,\n",
    "\tuse_cache=False,\n",
    "\tdevice_map=\"auto\",\n",
    "\tuse_gradient_checkpointing=True,\n",
    "\tlow_cpu_mem_usage=True,\n",
    "\ttrust_remote_code=True,\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.clean_up_tokenization_spaces = True\n",
    "tokenizer.chat_template = wandb.config[\"chat_template\"]\n",
    "tokenizer.add_special_tokens(wandb.config[\"special_tokens\"])\n",
    "base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262,184,960 || all params: 7,020,630,016 || trainable%: 3.7345\n"
     ]
    }
   ],
   "source": [
    "base_model_with_adapter = PeftModel.from_pretrained(base_model, wandb.config[\"adapter\"])\n",
    "model = torch.compile(base_model_with_adapter)\n",
    "FastLanguageModel.for_inference(model)\n",
    "base_model_with_adapter.print_trainable_parameters()\n",
    "FastLanguageModel.for_inference(base_model_with_adapter)\n",
    "\n",
    "# base_model_with_adapter = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "# \tbase_model_with_adapter,\n",
    "# \tdevice_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.map(lambda sample: {\n",
    "# \t\"input_ids\": tokenizer.apply_chat_template(\n",
    "# \t\tsample,\n",
    "# \t\ttokenize=True,\n",
    "# \t\tpadding=\"max_length\",\n",
    "# \t\tmax_length=wandb.config[\"max_input_tokens\"],\n",
    "# \t\tadd_generation_prompt=True,\n",
    "# \t\treturn_tensors=\"pt\"\n",
    "# \t)[0]\n",
    "# }, input_columns=\"query\", num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sentiment Analysis\n",
    "# analyser = pipeline(\n",
    "# \tmodel=wandb.config[\"sentiment_analysis_model\"],\n",
    "# \ttokenizer=wandb.config[\"sentiment_analysis_model\"],\n",
    "# \tmax_length=512,\n",
    "# \ttruncation=True,\n",
    "# \tframework=\"pt\",\n",
    "# \ttask=\"sentiment-analysis\",\n",
    "# \tnum_workers=16,\n",
    "# \tdevice_map=\"auto\",\n",
    "# \ttorch_dtype=\"auto\",\n",
    "# \tmodel_kwargs={\n",
    "# \t\t\"quantization_config\": BitsAndBytesConfig(\n",
    "# \t\t\tload_in_4bit=True,\n",
    "# \t\t\tbnb_4bit_compute_dtype=torch.float16\n",
    "# \t\t),\n",
    "# \t\t\"id2label\": {k: v for k, v in enumerate(emotion_labels)},\n",
    "# \t\t\"label2id\": {v: k for k, v in enumerate(emotion_labels)},\n",
    "# \t\t\"low_cpu_mem_usage\": True\n",
    "# \t},\n",
    "# \ttrust_remote_code=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_analysis_model = torch.compile(analyser.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_length = 69\n",
    "# def calculate_emotion_score(response: str, correct_emotion: str) -> float:\n",
    "#     # correct: save the score from analyser \n",
    "#     # wrong: [TO-DO] (save 1 - score from analyser )\n",
    "#     emotion_output = analyser(response)[0]\n",
    "#     if emotion_output[\"label\"] == correct_emotion:\n",
    "#         emotion_score = emotion_output[\"score\"] * 10\n",
    "#     else:\n",
    "#         emotion_score = 1 - emotion_output[\"score\"]\n",
    "#     return emotion_score\n",
    "\n",
    "# def calculate_length_score(response: str) -> float:\n",
    "#     # use reciprocal of length difference to calculate\n",
    "#     # the larger the difference the smaller the score is\n",
    "#     length_diff = abs(len(response) - target_length)\n",
    "#     print(length_diff)\n",
    "#     length_score = 1 / (length_diff + 1)\n",
    "#     return length_score\n",
    "\n",
    "# def reward(batch: dict) -> list:\n",
    "#     print(\"Hello Huston, here is a reward function\")\n",
    "#     correct_emotion = batch['query'][2]['content']['emotion']\n",
    "#     print(correct_emotion)\n",
    "#     rewards = []\n",
    "#     for response in batch[\"response\"]:\n",
    "#         emotion_score = calculate_emotion_score(response, correct_emotion)\n",
    "#         length_score = calculate_length_score(response)\n",
    "#         # use the product of two score as reward\n",
    "#         reward_product = emotion_score * length_score\n",
    "#         rewards.append(reward_product)\n",
    "    \n",
    "#     return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock_batch = {\n",
    "#     \"query\": [\n",
    "#         {\"content\": {\"text\": \"Hello, how are you?\", \"emotion\": \"sadness\"}},\n",
    "#         {\"content\": {\"text\": \"It's a nice day today.\", \"emotion\": \"neutral\"}},\n",
    "#         {\"content\": {\"text\": \"\", \"emotion\": \"anger\"}}\n",
    "#     ],\n",
    "#     \"response\": [\n",
    "#         \"I am doing well, thank you!\",\n",
    "#         \"Yes, it is a beautiful day.\",\n",
    "#         \"Man, wtf was that???? OK, I'm leaving now! Go fuck yourself and your bullshit.\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# rewards = reward(mock_batch)\n",
    "# print(\"Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo_config = PPOConfig(\n",
    "# \tgradient_accumulation_steps=1,\n",
    "# \tlearning_rate=wandb.config[\"learning_rate\"],\n",
    "# \tmax_grad_norm=wandb.config[\"max_grad_norm\"],\n",
    "# \tlog_with=\"wandb\",\n",
    "# \toptimize_device_cache=True,\n",
    "# \tearly_stopping=True,\n",
    "# \tis_peft_model=True,\n",
    "# \tuse_score_scaling=True,\n",
    "# \tuse_score_norm=True,\n",
    "# \tscore_clip=wandb.config[\"score_clip\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = PagedLion32bit(filter(lambda p: p.requires_grad, base_model.parameters()), lr=ppo_config.learning_rate)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bot = ResponseGeneratorPipeline(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     framework=\"pt\",\n",
    "#     task=\"conversation-generation\",\n",
    "#     num_workers=16,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     add_special_tokens=True,\n",
    "#     truncation=False,\n",
    "#     padding=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=16): 100%|██████████| 1024/1024 [00:00<00:00, 5466.73 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size after filter: 622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|██████████| 622/622 [00:00<00:00, 2475.60 examples/s]\n",
      "Map (num_proc=16): 100%|██████████| 622/622 [00:00<00:00, 3783.10 examples/s]\n",
      "Map (num_proc=16): 100%|██████████| 622/622 [00:00<00:00, 3552.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "dataset = load_dataset(\n",
    "\t\"hermeschen1116/daily_dialog_for_RG\",\n",
    "\tsplit=\"train+validation\",\n",
    "\tkeep_in_memory=True,\n",
    "\tnum_proc=16,\n",
    "\ttrust_remote_code=True\n",
    ")\n",
    "dataset = dataset.take(1024)   # use very small dataset to debug\n",
    "\n",
    "history_length: int = 2 * wandb.config[\"num_turns_history\"]\n",
    "dataset = dataset.filter(lambda sample: len(sample) >= (2 + history_length), input_columns=\"prompt\", num_proc=16)\n",
    "print(f\"dataset size after filter: {len(dataset)}\")\n",
    "\n",
    "dataset = dataset.map(lambda sample: {\n",
    "\t\"prompt\": sample[i: i + 2 + history_length]\n",
    "\tfor i in range(0, len(sample) - 2, 2) if (i + 2 + history_length) <= len(sample)\n",
    "}, input_columns=\"prompt\", num_proc=16)\n",
    "\n",
    "system_prompt: list = [{\"role\": \"system\", \"content\": {\"emotion\": \"\", \"dialog\": wandb.config[\"system_prompt\"]}}]\n",
    "\n",
    "dataset = dataset.map(lambda samples: {\n",
    "\t\"prompt\": [system_prompt + sample for sample in samples]\n",
    "}, input_columns=\"prompt\", batched=True, num_proc=16)\n",
    "\n",
    "emotion_labels: list = [\"neutral\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"]\n",
    "\n",
    "dataset = dataset.map(lambda samples: {\n",
    "\t\"query\": [\n",
    "\t\tsample[:-1] + [{\"role\": \"assistant\", \"content\": {\"emotion\": sample[-1][\"content\"][\"emotion\"], \"dialog\": \"\"}}]\n",
    "\t\tfor sample in samples\n",
    "\t],\n",
    "\t\"label\": [emotion_labels.index(sample[-1][\"content\"][\"emotion\"]) for sample in samples]\n",
    "}, input_columns=\"prompt\", remove_columns=\"prompt\", batched=True, num_proc=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': [{'content': {'dialog': '', 'emotion': ''}, 'role': 'system'},\n",
       "  {'content': {'dialog': \"I guess you are right.But what shall we do ? I don't feel like sitting at home .\",\n",
       "    'emotion': 'neutral'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': 'I suggest a walk over to the gym where we can play singsong and meet some of our friends .',\n",
       "    'emotion': 'neutral'},\n",
       "   'role': 'assistant'},\n",
       "  {'content': {'dialog': \"That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them .\",\n",
       "    'emotion': 'happiness'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': 'Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too .',\n",
       "    'emotion': 'happiness'},\n",
       "   'role': 'assistant'},\n",
       "  {'content': {'dialog': \"Good.Let ' s go now .\", 'emotion': 'happiness'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': '', 'emotion': 'happiness'}, 'role': 'assistant'}],\n",
       " 'label': 4}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|██████████| 622/622 [00:00<00:00, 2632.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda sample: {\n",
    "\t\"input_ids\": tokenizer.apply_chat_template(\n",
    "\t\tsample,\n",
    "\t\ttokenize=True,\n",
    "\t\tpadding=\"max_length\",\n",
    "\t\tmax_length=wandb.config[\"max_input_tokens\"],\n",
    "\t\tadd_generation_prompt=True,\n",
    "\t\treturn_tensors=\"pt\"\n",
    "\t)[0]\n",
    "}, input_columns=\"query\", num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': [{'content': {'dialog': '', 'emotion': ''}, 'role': 'system'},\n",
       "  {'content': {'dialog': \"I guess you are right.But what shall we do ? I don't feel like sitting at home .\",\n",
       "    'emotion': 'neutral'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': 'I suggest a walk over to the gym where we can play singsong and meet some of our friends .',\n",
       "    'emotion': 'neutral'},\n",
       "   'role': 'assistant'},\n",
       "  {'content': {'dialog': \"That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them .\",\n",
       "    'emotion': 'happiness'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': 'Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too .',\n",
       "    'emotion': 'happiness'},\n",
       "   'role': 'assistant'},\n",
       "  {'content': {'dialog': \"Good.Let ' s go now .\", 'emotion': 'happiness'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': '', 'emotion': 'happiness'}, 'role': 'assistant'}],\n",
       " 'label': 4,\n",
       " 'input_ids': [32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  ...]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Tokenizer\n",
    "# base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     wandb.config[\"base_model\"],\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     pretraining_tp=1,\n",
    "#     load_in_4bit=True,\n",
    "#     device_map=\"auto\",\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "# tokenizer.padding_side = \"left\"\n",
    "# tokenizer.clean_up_tokenization_spaces = True\n",
    "# tokenizer.chat_template = wandb.config[\"chat_template\"]\n",
    "# tokenizer.add_special_tokens(wandb.config[\"special_tokens\"])\n",
    "# base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# wandb.config[\"example_prompt\"] = tokenizer.apply_chat_template(dataset[0][\"prompt\"], tokenize=False)\n",
    "\n",
    "# model = PeftModel.from_pretrained(base_model, wandb.config[\"adapter\"])\n",
    "# model = torch.compile(model)\n",
    "# FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamer = TextStreamer(\n",
    "# \ttokenizer,\n",
    "# \tskip_special_tokens=True, # show <pad> or not\n",
    "# \tclean_up_tokenization_spaces=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation_config = GenerationConfig(\n",
    "# \tmax_length=(wandb.config[\"max_input_tokens\"] + wandb.config[\"max_new_tokens\"]),\n",
    "# \tmin_length=-1,\n",
    "# \ttop_k=wandb.config[\"top_k\"],\n",
    "# \ttop_p=wandb.config[\"top_p\"],\n",
    "# \t# do_sample=True,\n",
    "# \tuse_cache=True,\n",
    "# \trepetition_penalty=wandb.config[\"repetition_penalty\"],\n",
    "# \tpad_token_id=tokenizer.pad_token_id,\n",
    "# \tbos_token_id=tokenizer.bos_token_id,\n",
    "# \teos_token_id=tokenizer.eos_token_id,\n",
    "# \tlow_memory=True\n",
    "# )\n",
    "\n",
    "# generation_config = GenerationConfig(\n",
    "#     max_new_tokens=20,\n",
    "#     min_new_tokens=5,\n",
    "#     repetition_penalty=1.5,\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "#     eos_token_id=tokenizer.eos_token_id\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = dataset.map(lambda sample: {\n",
    "#     \"test_response\":\n",
    "#         bot(sample, generation_config=generation_config)[0][\"generated_text\"][-1][\"content\"][\"dialog\"]\n",
    "# }, input_columns=\"query\")\n",
    "# # result = result.remove_columns(\"prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels: list = [\"neutral\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"]\n",
    "analyser = pipeline(\n",
    "    model=\"Shotaro30678/emotion_text_classifier_on_dd_v1\",\n",
    "    framework=\"pt\",\n",
    "    task=\"sentiment-analysis\",\n",
    "    num_workers=16,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    model_kwargs={\n",
    "        \"quantization_config\": BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        ),\n",
    "        \"id2label\": {k: v for k, v in enumerate(emotion_labels)},\n",
    "        \"label2id\": {v: k for k, v in enumerate(emotion_labels)},\n",
    "        \"low_cpu_mem_usage\": True\n",
    "    },\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# result = result.add_column(\"test_response_sentiment\", analyser(result[\"test_response\"]))\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from libs import ResponseGeneratorPipeline\n",
    "# [TODO] a reward function contain length and emotion\n",
    "# target_length = wandb.config[\"target_length\"]\n",
    "target_length = 69\n",
    "\n",
    "# the length of output that we prefer\n",
    "\n",
    "def calculate_emotion_score(response: str, correct_emotion: str) -> float:\n",
    "    # correct: save the score from analyser \n",
    "    # wrong: [TO-DO] (save 1 - score from analyser )\n",
    "    emotion_output = analyser(response)[0]\n",
    "    print(emotion_output)\n",
    "    if emotion_output[\"label\"] == correct_emotion:\n",
    "        emotion_score = emotion_output[\"score\"] * 10\n",
    "    else:\n",
    "        emotion_score = 1 - emotion_output[\"score\"]\n",
    "    return emotion_score\n",
    "\n",
    "def calculate_length_score(response_length: int) -> float:\n",
    "    # use reciprocal of length difference to calculate\n",
    "    # the larger the difference the smaller the score is\n",
    "    length_diff = abs(response_length - target_length)\n",
    "    print(\"len and len diff\",response_length, length_diff)\n",
    "    length_score = 1 / (length_diff + 1)\n",
    "    return length_score\n",
    "\n",
    "def reward(batch: dict) -> list:\n",
    "    print(\"Hello Huston, here is a reward function\")\n",
    "    # correct_emotion = batch['label']\n",
    "    # print(batch)\n",
    "    # correct_emotion = emotion_labels[batch['label']]\n",
    "    # print(correct_emotion)\n",
    "    rewards = []\n",
    "    res_len = []\n",
    "    for response, response_length, raw_correct_emotion in zip(batch[\"response\"], batch[\"response_length\"], batch[\"label\"]):\n",
    "        # print(response, \"here\")\n",
    "        correct_emotion = emotion_labels[raw_correct_emotion]\n",
    "        res_len.append(len(response))\n",
    "        # print(response['test_response'])\n",
    "        # print(response['test_response_sentiment'])\n",
    "        # print(emotion_labels[response['label']], \"\\n\")\n",
    "        \n",
    "        emotion_score = calculate_emotion_score(response, correct_emotion)\n",
    "        length_score = calculate_length_score(response_length)\n",
    "        # use the product of two score as reward\n",
    "        reward_product = emotion_score * length_score\n",
    "        rewards.append(reward_product)\n",
    "    print(\"\\ntarget length: \", target_length)\n",
    "    print(\"test_response length\")\n",
    "    import statistics\n",
    "    print(\"max:\", max(res_len),\"\\nmin:\", min(res_len),\"\\navg:\", statistics.mean(res_len))\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for conversation-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Map: 100%|██████████| 622/622 [04:28<00:00,  2.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# reward function test code\n",
    "\n",
    "bot = ResponseGeneratorPipeline(\n",
    "    base_model_with_adapter,\n",
    "    tokenizer,\n",
    "    framework=\"pt\",\n",
    "    task=\"conversation-generation\",\n",
    "    num_workers=16,\n",
    "    torch_dtype=\"auto\",\n",
    "    add_special_tokens=True,\n",
    "    truncation=False,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=20,\n",
    "    min_new_tokens=5,\n",
    "    repetition_penalty=1.5,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "test_data = dataset.map(lambda sample: {\n",
    "\t\"query\": sample[\"query\"],\n",
    "\t\"label\": sample[\"label\"],\n",
    "\t\"input_ids\": sample[\"input_ids\"],\n",
    "\t\"response\": bot(sample[\"query\"], generation_config=generation_config)[0]['generated_token_ids']\n",
    "    # \"response_SHITS\"\n",
    "    # \"response\": bot(sample[\"query\"], streamer=streamer, generation_config=generation_config)[0][\"generated_text\"][-1][\"content\"][\"dialog\"]\n",
    "\n",
    "})\n",
    "\n",
    "# rewards: list = [reward(batch) for batch in DataLoader(test_data, batch_size=128)]\n",
    "# print(type(rewards[0])) # should be a list\n",
    "# print(rewards[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# class LengthSampler:\n",
    "#     def __init__(self, min_length: int, max_length: int):\n",
    "#         self.min_length = min_length\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __call__(self) -> int:\n",
    "#         return random.randint(self.min_length, self.max_length)\n",
    "\n",
    "from trl.core import LengthSampler\n",
    "length_sampler = LengthSampler(wandb.config[\"min_new_tokens\"], wandb.config[\"max_new_tokens\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.set_format(\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|██████████| 622/622 [00:00<00:00, 2439.75 examples/s]\n",
      "Map (num_proc=16): 100%|██████████| 622/622 [00:00<00:00, 2505.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = test_data.map(lambda sample: {\n",
    "\t\"response_token\": tokenizer.decode(\n",
    "     sample.squeeze(),\n",
    "     truncate=True,\n",
    "     max_length=70\n",
    "\t)\n",
    "}, input_columns=\"response\", num_proc=16)\n",
    "test_data = test_data.map(lambda sample: {\n",
    "\t\"response_length\": \n",
    "     sample.size()\n",
    "}, input_columns=\"response\", num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': [{'content': {'dialog': '', 'emotion': ''}, 'role': 'system'},\n",
       "  {'content': {'dialog': 'What position do you play ?', 'emotion': 'neutral'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': 'I ’ m a defender . It ’ s a lot of fun . You don ’ t have to be able to skate as fast on defense .',\n",
       "    'emotion': 'neutral'},\n",
       "   'role': 'assistant'},\n",
       "  {'content': {'dialog': 'Yeah , you ’ re a pretty big guy . I play goalie , myself .',\n",
       "    'emotion': 'neutral'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': 'Oh , yeah ? Which team ?', 'emotion': 'surprise'},\n",
       "   'role': 'assistant'},\n",
       "  {'content': {'dialog': 'The Rockets .', 'emotion': 'neutral'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': '', 'emotion': 'surprise'}, 'role': 'assistant'}],\n",
       " 'label': tensor(6),\n",
       " 'input_ids': tensor([32000, 32000, 32000,  ..., 29871, 32004,   259]),\n",
       " 'response': tensor([    1, 32001,   259, 32003, 29871, 21104, 29871, 32004, 29871,  1724,\n",
       "          2602,   437,   366,  1708,  1577, 29871, 32002,   259, 32003, 29871,\n",
       "         21104, 29871, 32004, 29871,   306, 16156,   286,   263,   822,  1581,\n",
       "           869,   739, 16156,   269,   263,  3287,   310,  2090,   869,   887,\n",
       "          1016, 16156,   260,   505,   304,   367,  2221,   304,  2071,   403,\n",
       "           408,  5172,   373, 26406,   869, 29871,     2, 29871,    13,     1,\n",
       "         32001,   259, 32003, 29871, 21104, 29871, 32004, 29871, 15011,  1919,\n",
       "           366, 16156,   337,   263,  5051,  4802,  1410, 29891,   869,   306,\n",
       "          1708,  7306,   347,  1919,  6142,   869, 29871, 32002,   259, 32003,\n",
       "         29871, 16671, 29871, 32004, 29871,  6439,  1919, 21915,  1577,  8449,\n",
       "          3815,  1577, 29871,     2, 29871,    13,     1, 32001,   259, 32003,\n",
       "         29871, 21104, 29871, 32004, 29871,   450,  8027,  1691,   869, 29871,\n",
       "         32002,   259, 32003, 29871, 16671, 29871, 32004,   259, 29900, 29882,\n",
       "          1738,  2193,  1818,  1207,   372,  5189,   363,   596, 11825,   746,\n",
       "           896,  1074,   278,  3748,   373,  5648,   470, 11621]),\n",
       " 'response_token': '<s>[INST]  [EMOTION]  neutral [/EMOTION]  What position do you play? [/INST]  [EMOTION]  neutral [/EMOTION]  I ’ m a defender. It ’ s a lot of fun. You don ’ t have to be able to skate as fast on defense. </s> \\n<s>[INST]  [EMOTION]  neutral [/EMOTION]  Yeah, you ’ re a pretty big guy. I play goalie, myself. [/INST]  [EMOTION]  surprise [/EMOTION]  Oh, yeah? Which team? </s> \\n<s>[INST]  [EMOTION]  neutral [/EMOTION]  The Rockets. [/INST]  [EMOTION]  surprise [/EMOTION]  0h! That must make it difficult for your parents when they see the game on TV or listen',\n",
       " 'response_length': tensor([148])}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [200] at entry 0 and [142] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 2\u001b[0m rewards: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(rewards[\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;66;03m# should be a list\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(rewards[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[103], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 2\u001b[0m rewards: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(rewards[\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;66;03m# should be a list\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(rewards[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chat-bot-20tW9agt-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chat-bot-20tW9agt-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chat-bot-20tW9agt-py3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chat-bot-20tW9agt-py3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chat-bot-20tW9agt-py3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:154\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m--> 154\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m}\u001b[49m)\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chat-bot-20tW9agt-py3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:154\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m--> 154\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate({key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chat-bot-20tW9agt-py3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chat-bot-20tW9agt-py3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [200] at entry 0 and [142] at entry 1"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "rewards: list = [reward(batch) for batch in DataLoader(test_data, batch_size=128)]\n",
    "print(type(rewards[0])) # should be a list\n",
    "print(rewards[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_length = 69\n",
    "# def calculate_emotion_score(response_sentiment: str, correct_emotion: str) -> float:\n",
    "#     # correct: save the score from analyser \n",
    "#     # wrong: [TO-DO] (save 1 - score from analyser )\n",
    "#     # emotion_output = analyser(response)[0]\n",
    "#     emotion_output = response_sentiment\n",
    "\n",
    "#     if emotion_output[\"label\"] == correct_emotion:\n",
    "#         emotion_score = emotion_output[\"score\"] * 10\n",
    "#     else:\n",
    "#         emotion_score = 1 - emotion_output[\"score\"]\n",
    "#     return emotion_score\n",
    "\n",
    "# def calculate_length_score(response: str) -> float:\n",
    "#     # print(\"len: \", len(response), \"res: \", response)\n",
    "#     # use reciprocal of length difference to calculate\n",
    "#     # the larger the difference the smaller the score is\n",
    "#     length_diff = abs(len(response) - target_length)\n",
    "#     # print(length_diff)\n",
    "#     length_score = 1 / (length_diff + 1)\n",
    "#     return length_score\n",
    "# import statistics\n",
    "# def reward(batch: dict) -> list:\n",
    "#     print(\"Hello Huston, here is a reward function\")\n",
    "#     # correct_emotion = batch['label']\n",
    "#     # print(batch)\n",
    "#     # print(correct_emotion)\n",
    "#     rewards = []\n",
    "#     res_len = []\n",
    "#     for response in batch:\n",
    "#         res_len.append(len(response['test_response']))\n",
    "#         # print(response['test_response'])\n",
    "#         # print(response['test_response_sentiment'])\n",
    "#         # print(emotion_labels[response['label']], \"\\n\")\n",
    "#         correct_emotion = emotion_labels[response['label']]\n",
    "#         emotion_score = calculate_emotion_score(response['test_response_sentiment'], correct_emotion)\n",
    "#         length_score = calculate_length_score(response['test_response'])\n",
    "#         # use the product of two score as reward\n",
    "#         reward_product = emotion_score * length_score\n",
    "#         rewards.append(reward_product)\n",
    "#     print(\"\\ntarget length: \", target_length)\n",
    "#     print(\"test_response length\")\n",
    "#     print(\"max:\", max(res_len),\"\\nmin:\", min(res_len),\"\\navg:\", statistics.mean(res_len))\n",
    "    \n",
    "#     return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_length = 69\n",
    "# def calculate_emotion_score(response: str, correct_emotion: str) -> float:\n",
    "#     # correct: save the score from analyser \n",
    "#     # wrong: [TO-DO] (save 1 - score from analyser )\n",
    "#     emotion_output = analyser(response)[0]\n",
    "#     print(emotion_output)\n",
    "#     if emotion_output[\"label\"] == correct_emotion:\n",
    "#         emotion_score = emotion_output[\"score\"] * 10\n",
    "#     else:\n",
    "#         emotion_score = 1 - emotion_output[\"score\"]\n",
    "#     return emotion_score\n",
    "\n",
    "# def calculate_length_score(response: str) -> float:\n",
    "#     # print(\"len: \", len(response), \"res: \", response)\n",
    "#     # use reciprocal of length difference to calculate\n",
    "#     # the larger the difference the smaller the score is\n",
    "#     length_diff = abs(len(response) - target_length)\n",
    "#     # print(length_diff)\n",
    "#     length_score = 1 / (length_diff + 1)\n",
    "#     return length_score\n",
    "# import statistics\n",
    "# def reward(batch: dict) -> list:\n",
    "#     print(\"Hello Huston, here is a reward function\")\n",
    "#     # correct_emotion = batch['label']\n",
    "#     # print(batch)\n",
    "#     # correct_emotion = emotion_labels[batch['label']]\n",
    "#     # print(correct_emotion)\n",
    "#     rewards = []\n",
    "#     res_len = []\n",
    "#     for response, raw_correct_emotion in zip(batch[\"response\"], batch[\"label\"]):\n",
    "#         # print(response, \"here\")\n",
    "#         correct_emotion = emotion_labels[raw_correct_emotion]\n",
    "#         res_len.append(len(response))\n",
    "#         # print(response['test_response'])\n",
    "#         # print(response['test_response_sentiment'])\n",
    "#         # print(emotion_labels[response['label']], \"\\n\")\n",
    "        \n",
    "#         emotion_score = calculate_emotion_score(response, correct_emotion)\n",
    "#         length_score = calculate_length_score(response)\n",
    "#         # use the product of two score as reward\n",
    "#         reward_product = emotion_score * length_score\n",
    "#         rewards.append(reward_product)\n",
    "#     print(\"\\ntarget length: \", target_length)\n",
    "#     print(\"test_response length\")\n",
    "#     print(\"max:\", max(res_len),\"\\nmin:\", min(res_len),\"\\navg:\", statistics.mean(res_len))\n",
    "    \n",
    "#     return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response, raw_correct_emotion in zip(batch[\"input_ids\"], batch[\"label\"]):\n",
    "        # print(response, \"here\")\n",
    "        correct_emotion = emotion_labels[raw_correct_emotion]\n",
    "        print(correct_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wards = reward(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock_batch = {\n",
    "#     \"query\": [\n",
    "#         {\"content\": {\"text\": \"Hello, how are you?\", \"emotion\": \"sadness\"}},\n",
    "#         {\"content\": {\"text\": \"It's a nice day today.\", \"emotion\": \"neutral\"}},\n",
    "#         {\"content\": {\"text\": \"\", \"emotion\": \"anger\"}}\n",
    "#     ],\n",
    "#     \"response\": [\n",
    "#         \"I am doing well, thank you!\",\n",
    "#         \"Yes, it is a beautiful day.\",\n",
    "#         \"Man, wtf was that???? OK, I'm leaving now! Go fuck yourself and your bullshit.\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# rewards = reward(mock_batch)\n",
    "# print(\"Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards = reward(result)\n",
    "# print(\"\\nRewards len:\", len(rewards))\n",
    "# print(\"Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup Tuner\n",
    "# tuner = PPOTrainer(\n",
    "# \tconfig=ppo_config,\n",
    "# \tmodel=base_model_with_adapter,\n",
    "# \ttokenizer=tokenizer,\n",
    "# \tdataset=dataset,\n",
    "# \toptimizer=optimizer,\n",
    "# \tlr_scheduler=lr_scheduler\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in trange(1, colour=\"blue\"):\n",
    "# \tfor batch in tqdm(tuner.dataloader, colour=\"yellow\"):\n",
    "# \t\tquery_tensors = batch[\"input_ids\"] # somehow has 2048 ids\n",
    "# \t\tbreak\n",
    "# \t\tresponse_tensors = tuner.generate(\n",
    "# \t\t\tquery_tensors,\n",
    "# \t\t\treturn_prompt=False,\n",
    "# \t\t\tbatch_size=1,   # must set to 1 if using streamer\n",
    "# \t\t\tstreamer=streamer,  # use streamer to show the generation process\n",
    "# \t\t\t**generation_config.to_dict()\n",
    "# \t\t)\n",
    "# \t\tbatch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "# \t\tresponse_tensors = [torch.LongTensor(t.to(\"cpu\")) for t in response_tensors]\n",
    "\n",
    "# \t\treward_scores = reward(batch)\n",
    "# \t\trewards = [torch.FloatTensor(torch.tensor(scores, device=\"cpu\")) for scores in reward_scores]\n",
    "\n",
    "# \t\tstats = tuner.step(query_tensors, response_tensors, rewards)\n",
    "# \t\ttuner.log_stats(stats, batch, rewards)\n",
    "\n",
    "# # model_artifact = wandb.Artifact(\n",
    "# # \twandb.config[\"fine_tuned_model\"],\n",
    "# # \ttype=\"model\"\n",
    "# # )\n",
    "\n",
    "# # tuner.model = torch.compile(tuner.model)\n",
    "# # tuner.model.push_to_hub(repo_id=\"response_generator_for_emotion_chat_bot\", commit=\"\", create_pr=True)\n",
    "# # with tempfile.TemporaryDirectory() as temp_dir:\n",
    "# # \ttuner.model.save_pretrained(temp_dir, save_embedding_layers=True)\n",
    "# # \tmodel_artifact.add_dir(temp_dir)\n",
    "# # \trun.log_artifact(model_artifact)\n",
    "\n",
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-bot-20tW9agt-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
